{"cells":[{"metadata":{"collapsed":true},"cell_type":"code","source":"################################################################################\n#\n# Licensed Materials - Property of IBM\n# (C) Copyright IBM Corp. 2019\n# US Government Users Restricted Rights - Use, duplication disclosure restricted\n# by GSA ADP Schedule Contract with IBM Corp.\n#\n################################################################################\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## IBM AutoAI Auto-Generated Notebook v1.10.2\n### Representing Pipeline: P3 from run 9ac2480f-c953-4ac7-9356-1ca5acd626ae\n\n**Note**: Notebook code generated using AutoAI will execute successfully.  If code is modified or reordered, there is no guarantee it will successfully execute, please read our documentation for more information https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/autoai-notebook.html .\n\nBefore modifying the pipeline or trying to re-fit the pipeline, consider:\nThe notebook converts dataframes to numpy arrays before fitting the pipeline (a current restriction of the preprocessor pipeline).\nThe known_values_list is passed by reference and populated with categorical values during fit of the preprocessing pipeline.  Delete its members before re-fitting.\n"},{"metadata":{},"cell_type":"markdown","source":"### 1. Set Up"},{"metadata":{"collapsed":true},"cell_type":"code","source":"import sklearn\ntry:\n    import xgboost\nexcept:\n    print('xgboost, if needed, will be installed and imported later')\ntry:\n    import lightgbm\nexcept:\n    print('lightgbm, if needed, will be installed and imported later')\nfrom sklearn.cluster import FeatureAgglomeration\nimport numpy\nfrom numpy import nan, dtype, mean\nimport autoai_libs\nfrom autoai_libs.sklearn.custom_scorers import CustomScorers\nimport sklearn.ensemble\nfrom autoai_libs.cognito.transforms.transform_utils import TExtras, FC\nfrom autoai_libs.transformers.exportable import *\nfrom autoai_libs.utils.exportable_utils import *\nfrom sklearn.pipeline import Pipeline\nknown_values_list=[]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Compose Pipeline"},{"metadata":{"collapsed":true},"cell_type":"code","source":"#Load pipeline while attempting to automatically import modules and install missing packages as necessary.\nretries = 0\nsuccessful = False\nwhile retries < 10 and not successful:\n    retries += 1\n    try:\n        #\n        # composing steps for toplevel Pipeline\n        #\n        _input_metadata = {'run_uid': '9ac2480f-c953-4ac7-9356-1ca5acd626ae', 'pn': 'P3', 'data_source': '', 'target_label_name': 'CHURN', 'learning_type': 'classification', 'optimization_metric': 'roc_auc', 'random_state': 33, 'cv_num_folds': 3, 'holdout_fraction': 0.1, 'data_provenance': {'input_data': [{'id': '1', 'type': 's3', 'connection': {'access_key_id': '05f78ee8880c48f089b73ca4e5114336', 'secret_access_key': 'a5f8a7d04a2a825bc7c908f774566deedf37af33d31a543f', 'endpoint_url': 'https://s3.eu-geo.objectstorage.softlayer.net'}, 'location': {'type': 's3', 'bucket': 'bnpparibasbu20191016-donotdelete-pr-nvjrdigcemyqdh', 'path': 'data_asset/customer_churn.csv_shaped_e1744397.csv'}}]}}\n        steps=[]\n        #\n        # composing steps for preprocessor Pipeline\n        #\n        preprocessor__input_metadata = None\n        preprocessor_steps=[]\n        #\n        # composing steps for preprocessor_features FeatureUnion\n        #\n        preprocessor_features_transformer_list=[]\n        #\n        # composing steps for preprocessor_features_categorical Pipeline\n        #\n        preprocessor_features_categorical__input_metadata = None\n        preprocessor_features_categorical_steps=[]\n        preprocessor_features_categorical_steps.append(('cat_column_selector', autoai_libs.transformers.exportable.NumpyColumnSelector(columns=[0, 1, 2, 4, 5])))\n        preprocessor_features_categorical_steps.append(('cat_compress_strings', autoai_libs.transformers.exportable.CompressStrings(activate_flag=True,\n                dtypes_list=['char_str', 'char_str', 'int_num', 'char_str', 'int_num'],\n                missing_values_reference_list=['', nan, '?', '-'],\n                misslist_list=[[], [], [], [], []])))\n        preprocessor_features_categorical_steps.append(('cat_missing_replacer', autoai_libs.transformers.exportable.NumpyReplaceMissingValues(filling_values=nan, missing_values=[])))\n        preprocessor_features_categorical_steps.append(('cat_unknown_replacer', autoai_libs.transformers.exportable.NumpyReplaceUnknownValues(filling_values=nan,\n                     filling_values_list=[nan, nan, nan, nan, nan],\n                     known_values_list=known_values_list,\n                     missing_values_reference_list=['', nan, '?', '-'])))\n        preprocessor_features_categorical_steps.append(('boolean2float_transformer', autoai_libs.transformers.exportable.boolean2float(activate_flag=True)))\n        preprocessor_features_categorical_steps.append(('cat_imputer', autoai_libs.transformers.exportable.CatImputer(activate_flag=True, missing_values=nan,\n              sklearn_version_family='20', strategy='most_frequent')))\n        preprocessor_features_categorical_steps.append(('cat_encoder', autoai_libs.transformers.exportable.CatEncoder(activate_flag=True, categories='auto',\n              dtype=numpy.float64, encoding='ordinal',\n              handle_unknown='error', sklearn_version_family='20')))\n        preprocessor_features_categorical_steps.append(('float32_transformer', autoai_libs.transformers.exportable.float32_transform(activate_flag=True)))\n        # assembling preprocessor_features_categorical_ Pipeline\n        preprocessor_features_categorical_pipeline = sklearn.pipeline.Pipeline(steps=preprocessor_features_categorical_steps)\n        preprocessor_features_transformer_list.append(('categorical', preprocessor_features_categorical_pipeline))\n        #\n        # composing steps for preprocessor_features_numeric Pipeline\n        #\n        preprocessor_features_numeric__input_metadata = None\n        preprocessor_features_numeric_steps=[]\n        preprocessor_features_numeric_steps.append(('num_column_selector', autoai_libs.transformers.exportable.NumpyColumnSelector(columns=[3])))\n        preprocessor_features_numeric_steps.append(('num_floatstr2float_transformer', autoai_libs.transformers.exportable.FloatStr2Float(activate_flag=True, dtypes_list=['float_num'],\n                missing_values_reference_list=[])))\n        preprocessor_features_numeric_steps.append(('num_missing_replacer', autoai_libs.transformers.exportable.NumpyReplaceMissingValues(filling_values=nan, missing_values=[])))\n        preprocessor_features_numeric_steps.append(('num_imputer', autoai_libs.transformers.exportable.NumImputer(activate_flag=True, missing_values=nan, strategy='median')))\n        preprocessor_features_numeric_steps.append(('num_scaler', autoai_libs.transformers.exportable.OptStandardScaler(num_scaler_copy=None, num_scaler_with_mean=None,\n                 num_scaler_with_std=None, use_scaler_flag=False)))\n        preprocessor_features_numeric_steps.append(('float32_transformer', autoai_libs.transformers.exportable.float32_transform(activate_flag=True)))\n        # assembling preprocessor_features_numeric_ Pipeline\n        preprocessor_features_numeric_pipeline = sklearn.pipeline.Pipeline(steps=preprocessor_features_numeric_steps)\n        preprocessor_features_transformer_list.append(('numeric', preprocessor_features_numeric_pipeline))\n        # assembling preprocessor_features_ FeatureUnion\n        preprocessor_features_pipeline = sklearn.pipeline.FeatureUnion(transformer_list=preprocessor_features_transformer_list)\n        preprocessor_steps.append(('features', preprocessor_features_pipeline))\n        preprocessor_steps.append(('permuter', autoai_libs.transformers.exportable.NumpyPermuteArray(axis=0, permutation_indices=[0, 1, 2, 4, 5, 3])))\n        # assembling preprocessor_ Pipeline\n        preprocessor_pipeline = sklearn.pipeline.Pipeline(steps=preprocessor_steps)\n        steps.append(('preprocessor', preprocessor_pipeline))\n        #\n        # composing steps for cognito Pipeline\n        #\n        cognito__input_metadata = None\n        cognito_steps=[]\n        cognito_steps.append(('0', autoai_libs.cognito.transforms.transform_utils.TA2(fun = numpy.true_divide, name = 'divide', datatypes1 = ['intc', 'intp', 'int_', 'uint8', 'uint16', 'uint32', 'uint64', 'int8', 'int16', 'int32', 'int64', 'short', 'long', 'longlong', 'float16', 'float32', 'float64'], feat_constraints1 = [FC.is_not_categorical], datatypes2 = ['intc', 'intp', 'int_', 'uint8', 'uint16', 'uint32', 'uint64', 'int8', 'int16', 'int32', 'int64', 'short', 'long', 'longlong', 'float16', 'float32', 'float64'], feat_constraints2 = [FC.is_not_categorical], tgraph = None, apply_all = True, col_names = ['Gender', 'Status', 'Children', 'Est Income', 'Car Owner', 'Age'], col_dtypes = [dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32')])))\n        cognito_steps.append(('1', autoai_libs.cognito.transforms.transform_utils.FS1(cols_ids_must_keep = range(0, 6), additional_col_count_to_keep = 8, ptype = 'classification')))\n        # assembling cognito_ Pipeline\n        cognito_pipeline = sklearn.pipeline.Pipeline(steps=cognito_steps)\n        steps.append(('cognito', cognito_pipeline))\n        steps.append(('estimator', lightgbm.sklearn.LGBMClassifier(boosting_type='gbdt', class_weight='balanced',\n                colsample_bytree=1.0, importance_type='split', learning_rate=0.1,\n                max_depth=-1, min_child_samples=20, min_child_weight=0.001,\n                min_split_gain=0.0, n_estimators=100, n_jobs=1, num_leaves=31,\n                objective=None, random_state=33, reg_alpha=0.0, reg_lambda=0.0,\n                silent=True, subsample=1.0, subsample_for_bin=200000,\n                subsample_freq=0)))\n        # assembling  Pipeline\n        pipeline = sklearn.pipeline.Pipeline(steps=steps)\n\n        successful = True\n    except Exception as e:\n        estr = str(e)\n        if estr.startswith('name ') and estr.endswith(' is not defined'):\n            try:\n                import importlib\n                module_name = estr.split(\"'\")[1]\n                module = importlib.import_module(module_name)\n                globals().update({module_name: module})\n            except Exception as import_failure:\n                print('import of ' + module_name + ' failed with: ' + str(import_failure))\n                import subprocess\n                print('attempting pip install of ' + module_name)\n                process = subprocess.Popen('pip install ' + module_name, shell=True)\n                process.wait()\n                try:\n                    print('re-attempting import of ' + module_name)\n                    module = importlib.import_module(module_name)\n                    globals().update({module_name: module})\n                    print('import successful for ' + module_name)\n                except Exception as import_or_installation_failure:\n                    print('failure installing and/or importing ' + module_name + ' error was: ' + str(\n                        import_or_installation_failure))\n                    raise (ModuleNotFoundError('Missing package in environment for ' + module_name +\n                        '? Try import and/or pip install manually?'))\n        else:\n            raise(e)\nif successful:\n    print('Pipeline successfully instantiated')\nelse:\n    raise (ModuleNotFoundError('Remaining missing imports/packages in environment? Retry cell and/or try pip install manually?'))\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true},"cell_type":"code","source":"# Metadata used in retrieving data and computing metrics.  Customize as necessary for your environment.\ndata_source='replace_with_path_and_csv_filename'\ntarget_label_name = _input_metadata['target_label_name']\nlearning_type = _input_metadata['learning_type']\noptimization_metric = _input_metadata['optimization_metric']\nrandom_state = _input_metadata['random_state']\ncv_num_folds = _input_metadata['cv_num_folds']\nholdout_fraction = _input_metadata['holdout_fraction']\ndata_provenance = _input_metadata['data_provenance']\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true},"cell_type":"code","source":"import pandas as pd\ndf = None\nif type(data_provenance) is str:\n    try:\n        df = pd.read_csv(data_provenance) # your data file name here\n    except Exception as e:\n        print(e)\nif df is None:\n    try:\n        data_location = data_provenance['input_data'][0]\n        print('data_location '+ str(data_location))\n        import boto3\n        session = boto3.session.Session()\n        cos = session.client(\n            service_name='s3',\n            aws_access_key_id=data_location['connection']['access_key_id'],\n            aws_secret_access_key=data_location['connection']['secret_access_key'],\n            endpoint_url=data_location['connection']['endpoint_url'],\n            verify=False\n        )\n        local_path = data_location['location']['path']\n        print('local_path ' + str(local_path))\n        cos.download_file(data_location['location']['bucket'],\n                     data_location['location']['path'],\n                     local_path)\n        df = pd.read_csv(local_path) # your data file name here\n    except Exception as e:\n        print(e)\nif df is None:\n    raise(ValueError('need location or credential information to read dataframe from COS'))\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true},"cell_type":"code","source":"target = target_label_name # your target name here\ndf_y = df[target]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":"df_X = df.drop(columns=[target])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true},"cell_type":"code","source":"# until the problem type is available in the metadata, use the sklearn type_of_target to determine whether to stratify the holdout split\nfrom sklearn.utils.multiclass import type_of_target\nif type_of_target(df_y.values) in ['multiclass', 'binary']:\n    X, X_holdout, y, y_holdout = train_test_split(df_X.values, df_y.values, test_size=holdout_fraction, random_state=random_state, stratify=df_y.values)\nelse:\n    X, X_holdout, y, y_holdout = train_test_split(df_X.values, df_y.values, test_size=holdout_fraction, random_state=random_state)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":"pipeline.fit(X, y)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":"y_pred = pipeline.predict(X_holdout)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":"print(y_holdout)\nprint(y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":"from sklearn.metrics import get_scorer\nscorer = get_scorer(optimization_metric)\nscorer(pipeline, X_holdout, y_holdout)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":"#delete CONTENTS of known_values_list before refitting, cloning or cross_validate-ing the pipeline, or previous values will be used.\nfor i in range(len(known_values_list)):\n    del known_values_list[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":"from sklearn.model_selection import cross_validate\ncv_results = cross_validate(pipeline, X, y, scoring={optimization_metric:scorer})\nimport numpy as np\nnp.mean(cv_results['test_' + optimization_metric])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"code","source":"cv_results","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3.6","language":"python"},"language_info":{"name":"python","version":"3.6.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}